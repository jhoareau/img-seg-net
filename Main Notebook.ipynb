{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic image segmentation using fully convolutional networks\n",
    "\n",
    "> Michal Gallus (s172679) Julien Hoareau (s161088) Wazir Sahebali (s172062)\n",
    "\n",
    "This notebook will walk you through the steps to be taken for producing the eventual evaluation results. Below, the test set of the [CamVid dataset](https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid) will be fed to our implementation of the 56-layer Fully Convolutional DenseNet.\n",
    "\n",
    "Our notebook on human segmentation within the ADE20K dataset can be found [here](./ADE%20Notebook.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can specify the amount of test images you want to go through the model. The maximum amount is 233, as there are only that many test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 3 # total 233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "We import the main libraries along with the 56 layer version of the network and its parameters. The input images all have a size of 360 by 480 pixels and will be resized to a 224 by 224 resolution. In this application the network has 12 classes (if the void class is included). In the next cell we also specify those classes along with their respective RGB colour code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from network import net\n",
    "import json\n",
    "from data_utils import *\n",
    "from colorise_camvid import colorize, legend, _mask_labels\n",
    "from iou_calculation import intersection_over_union\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# Network specifications\n",
    "batch_size, height, width, nchannels = n_images, 360, 480, 3\n",
    "final_resized = 224\n",
    "model_version = 56\n",
    "\n",
    "# Mapping of the classes onto colours and strings\n",
    "_cmap = [(128, 128, 128), (128, 0, 0), (192, 192, 128), (128, 64, 128), (0, 0, 192), (128, 128, 0),(192, 128, 128), (64, 64, 128), (64, 0, 128), (64, 64, 0), (0, 128, 192), (0, 0, 0)]\n",
    "_mask_labels = {0: 'sky', 1: 'building', 2: 'column_pole', 3: 'road',\n",
    "                4: 'sidewalk', 5: 'tree', 6: 'sign', 7: 'fence', 8: 'car',\n",
    "                9: 'pedestrian', 10: 'byciclist', 11: 'void'}\n",
    "\n",
    "# Model parameters\n",
    "with open('model_parameters.json') as params:\n",
    "    params_dict = json.load(params)[repr(model_version)]\n",
    "\n",
    "params_dict['input_num_features'] = 48\n",
    "params_dict['output_classes'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "The Tensorflow record file is actually already in the GitHub folder, so there is no need to recreate them. Nevertheless, we put the code for recreating them below as well. If you want to recreate the record file, then don't forget to adapt the file path of the CamVid folder (the variable `camvid_path`) to your situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF RECORD creation code\n",
    "camvid_path = \"../../CamVid\"  # Path to the CamVid dataset\n",
    "path_separator = \"/\"  \n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import os\n",
    "import random\n",
    "\n",
    "\"\"\"Based on: http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/21/tfrecords-guide/\n",
    "   and https://kwotsin.github.io/tech/2017/01/29/tfrecords.html\"\"\"\n",
    "\n",
    "# Functions to store images, integers and strings in Tensorrec format\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "reader = tf.TFRecordReader\n",
    "\n",
    "# The image annotation pairs have been coupled together in a text file\n",
    "# So we need to parse those text files\n",
    "original_images = []\n",
    "test_paths = camvid_path + path_separator + \"test.txt\"\n",
    "train_paths = camvid_path + path_separator + \"train.txt\"\n",
    "valid_paths = camvid_path + path_separator + \"val.txt\"\n",
    "\n",
    "def parsepaths(location):\n",
    "    # Load image and notations in separate lists\n",
    "    x = []\n",
    "    y = []\n",
    "    n = []\n",
    "    with open(location) as f:\n",
    "        data = f.read()\n",
    "    f.closed\n",
    "    for l in data.split(\"\\n\"):\n",
    "        if len(l) > 17:  # If the line contains more characters than the typical filename of one sample image\n",
    "            x.append(path_replace(l.split(\" \")[0]))\n",
    "            y.append(path_replace(l.split(\" \")[1]))\n",
    "            n.append(l.split(\" \")[0].replace(\"/SegNet/CamVid/train/\", \"\"))\n",
    "    return zip(x, y, n)\n",
    "\n",
    "def path_replace(path):\n",
    "    path = path.replace(\"/SegNet/CamVid/\", \"../\")\n",
    "    path = path.replace(\"/\", path_separator)\n",
    "    path = path.replace(\"..\", camvid_path)\n",
    "    return path\n",
    "\n",
    "# This is the main function which saves the images of the selected dataset to a tfrec format\n",
    "def tfrec_dump(dataset_paths, save_path):  # Either test_paths, train_paths or valid_paths\n",
    "    filename_pairs = parsepaths(dataset_paths)\n",
    "    writer = tf.python_io.TFRecordWriter(save_path)\n",
    "    for img_path, annotation_path, file_name in filename_pairs:\n",
    "        img = tf.gfile.FastGFile(img_path, 'rb').read()\n",
    "        annotation = tf.gfile.FastGFile(annotation_path, 'rb').read()\n",
    "        imgarr = np.array(Image.open(img_path))\n",
    "        height = imgarr.shape[0]\n",
    "        width = imgarr.shape[1]\n",
    "        original_images.append((img, annotation))\n",
    "\n",
    "        # Because the image is stored 1D we need to keep track of the image width and height\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'image/format': _bytes_feature(file_name[-3:].encode('ascii')),\n",
    "            'image/height': _int64_feature(height),\n",
    "            'image/width': _int64_feature(width),\n",
    "            'file_name': _bytes_feature(file_name.encode('ascii')),\n",
    "            'image/encoded': _bytes_feature(img),\n",
    "            'annotation/encoded': _bytes_feature(annotation)}))  # We assume here that the other features of the annotation image are the same as for the photo image\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    \n",
    "# The following functions defines how to decode the dataset file\n",
    "def slim_dataset(tfrec_location, num_samples):\n",
    "    # How to interpret the dict keys\n",
    "    keys_to_features = {\n",
    "        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'annotation/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n",
    "    }\n",
    "\n",
    "    # How to decode certain keys\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image(),\n",
    "        'annotation': slim.tfexample_decoder.Image(image_key='annotation/encoded', format_key='image/format', channels=1),\n",
    "    }\n",
    "    items_to_descriptions = {\n",
    "        'image': 'A 3-channel RGB coloured street image.',\n",
    "        'annotation': 'A 1-channel image where everything is annotated.'\n",
    "    }\n",
    "\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources=tfrec_location,\n",
    "        decoder=decoder,\n",
    "        reader=reader,\n",
    "        num_readers=4,\n",
    "        num_samples=num_samples,\n",
    "        items_to_descriptions=items_to_descriptions)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can read in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetfilename=\"testset.tfrec\"\n",
    "if not os.path.isfile(datasetfilename):\n",
    "        tfrec_dump(test_paths, datasetfilename)\n",
    "tfsdataset = slim_dataset(datasetfilename, n_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the network to load the images, it has to perform several operations on them. The image has to be randomly cropped and randomly flipped. In order to perform this in the same way for the image and the annotation the annotation is concatenated as an extra feature map to the image. Then these data augmentation methods are performed and the image is separated again from its annotation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip_crop_image_and_labels(image, labels, feature_maps_image, feature_maps_annot, height, width):\n",
    "    \"\"\"Randomly crops `image` together with `labels`.\n",
    "    Based on <https://stackoverflow.com/questions/42147427/tensorflow-how-to-randomly-crop-input-images-and-labels-in-the-same-way>\n",
    "    Args:\n",
    "    image: A Tensor with shape [D_1, ..., D_K, N]\n",
    "    labels: A Tensor with shape [D_1, ..., D_K, M]\n",
    "    size: A Tensor with shape [K] indicating the crop size.\n",
    "    Returns:\n",
    "    A tuple of (cropped_image, cropped_label).\n",
    "    \"\"\"\n",
    "    seed = random.randint(0, 1e10)\n",
    "    combined = tf.concat([image, labels], axis=-1)\n",
    "\n",
    "    last_label_dim = tf.shape(labels)[-1]\n",
    "    last_image_dim = tf.shape(image)[-1]\n",
    "    combined_crop = tf.random_crop(\n",
    "        combined,\n",
    "        size=[height, width, feature_maps_image + feature_maps_annot],\n",
    "        seed=seed)\n",
    "    combined_crop = tf.reshape(combined_crop, shape=(height, width, feature_maps_image + feature_maps_annot))\n",
    "    maybe_flipped_images = tf.image.random_flip_left_right(combined_crop)\n",
    "    crop_feature_maps = tf.unstack(maybe_flipped_images, axis=-1)\n",
    "    return tf.stack(crop_feature_maps[:feature_maps_image], axis=-1), tf.stack(crop_feature_maps[feature_maps_image:], axis=-1)\n",
    "\n",
    "# Convert to a tensor and resize\n",
    "def imagepreprocessor(image, annot, height, width, scope=None):\n",
    "    scopename=\"crop\"\n",
    "    with tf.name_scope(scope, scopename, [image, annot, height, width]):\n",
    "        if image.dtype != tf.float32:\n",
    "            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        if annot.dtype != tf.float32:\n",
    "            annot = tf.image.convert_image_dtype(annot, dtype=tf.float32)\n",
    "        image, annot = random_flip_crop_image_and_labels(\n",
    "                                image, annot,\n",
    "                                feature_maps_image=3,\n",
    "                                feature_maps_annot=1,\n",
    "                                height=height,\n",
    "                                width=width)\n",
    "    return tf.expand_dims(image, 0), tf.expand_dims(tf.image.convert_image_dtype(annot, dtype=tf.uint8), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function brings together the data loading and dataprocessing, so that the tensors returned by this function can be directly fed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch\n",
    "def batch(dataset, batch_size=3, height=360, width=480, resized=224):  # Resize to a multiple of 32\n",
    "    IMAGE_HEIGHT = IMAGE_WIDTH = resized\n",
    "    # First create the data_provider object\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        common_queue_capacity=24 + 3 * batch_size,\n",
    "        common_queue_min=24)\n",
    "\n",
    "    # Get the images from provider\n",
    "    raw_image, raw_annotation = data_provider.get(['image', 'annotation'])\n",
    "\n",
    "    # Do image preprocessing\n",
    "    image, annotation = imagepreprocessor(\n",
    "        image=raw_image, annot=raw_annotation, height=IMAGE_HEIGHT, width=IMAGE_WIDTH)\n",
    "\n",
    "    # Loaded batch\n",
    "    images, annotations = tf.train.batch(\n",
    "        [image, annotation],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=4,\n",
    "        capacity=4 * batch_size,\n",
    "        allow_smaller_final_batch=True)\n",
    "    return images, annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Network\n",
    "The network contains the following building blocks:\n",
    "* Dense blocks\n",
    "    - Skipped\n",
    "    - Non-skipped\n",
    "* Transition layers\n",
    "    - Transition up\n",
    "    - Transition down\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the hyperparameters along with the batch normalization and the creation of a standard layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = slim.l2_regularizer(0.0001)\n",
    "regularizers = {\"beta\" : slim.l2_regularizer(0.0001), \"gamma\": slim.l2_regularizer(0.0001)}\n",
    "weight_initializer = tf.contrib.keras.initializers.he_uniform()\n",
    "\n",
    "def batch_wise_batch_norm(x, scope):\n",
    "    with tf.name_scope(scope):\n",
    "        batch_mean, batch_var = tf.nn.moments(x, axes=[0, 1, 2])\n",
    "        x = tf.subtract(x, batch_mean)\n",
    "        x = tf.div(x, tf.sqrt(batch_var) + 1e-6)\n",
    "        x = tf.nn.relu(x, \"BatchNormRelu\")\n",
    "        return x\n",
    "def create_layer(input, num_features, scope, kernel_size=3, p=0.2, is_training=True):\n",
    "    relud_batch_norm = batch_wise_batch_norm(input, scope + \"/batchnorm\")\n",
    "    conv = slim.conv2d(relud_batch_norm, num_features,\n",
    "                       kernel_size, weights_initializer=weight_initializer,\n",
    "                       weights_regularizer=l2_reg,\n",
    "                       scope=(scope + \"/conv\"), activation_fn=None)\n",
    "    dropout = slim.dropout(conv, keep_prob=1-p, scope=(scope + \"/dropout\"), is_training=is_training)\n",
    "    return dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipped_dense_block(x, num_layers, num_features, scope, p=0.2, is_training=True):\n",
    "    layers = []\n",
    "    for i in range(num_layers):\n",
    "        layer = create_layer(x, num_features, p=p,\n",
    "                             scope=(scope + \"/layer\" + str(i)), is_training=is_training)\n",
    "        layers.append(layer)\n",
    "        x = tf.concat(axis=-1, values=[x, layer],\n",
    "                      name=(scope + \"/skip\" + str(i)))\n",
    "    return x\n",
    "\n",
    "def nonskipped_dense_block(x, num_layers, num_features, scope, p=0.2, is_training=True):\n",
    "    layers = []\n",
    "    for i in range(num_layers):\n",
    "        layer = create_layer(x, num_features, p=p,\n",
    "                             scope=(scope + \"/layer\" + str(i)), is_training=is_training)\n",
    "        layers.append(layer)\n",
    "        if (i == num_layers - 1):\n",
    "            continue\n",
    "        x = tf.concat(axis=-1, values=[x, layer],\n",
    "                      name=(scope + \"/skip\" + str(i)))\n",
    "    return tf.concat(axis=-1, values=layers,\n",
    "                    name=(scope + '/output'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_down(input, scope, kernel_size=1, pool_size=2, p=0.2, is_training=True):\n",
    "    relud_batch_norm = batch_wise_batch_norm(input, scope + \"/batchnorm\")\n",
    "    # relud_batch_norm = slim.batch_norm(input, activation_fn=tf.nn.relu,\n",
    "    #      param_regularizers=regularizers, scope=(scope + \"/batchnorm\"))\n",
    "    conv = slim.conv2d(relud_batch_norm, input.shape[-1],\n",
    "                       kernel_size, weights_initializer=weight_initializer,\n",
    "                       weights_regularizer=l2_reg,\n",
    "                       scope=(scope + \"/conv\"), activation_fn=None)\n",
    "    dropout = slim.dropout(conv, keep_prob=1-p, scope=(scope + \"/dropout\"), is_training=is_training)\n",
    "    max_pool = slim.max_pool2d(\n",
    "        dropout, pool_size, stride=2, scope=(scope + \"/maxpool\"))\n",
    "    return max_pool\n",
    "\n",
    "\n",
    "def transition_up(input, scope, kernel_size=3, stride=2):\n",
    "    return slim.conv2d_transpose(input, input.shape[-1], kernel_size,\n",
    "                                 weights_initializer=weight_initializer,\n",
    "                                 weights_regularizer=l2_reg,\n",
    "                                 stride=stride, scope=scope, activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network starts with a 3 by 3 convolution of the input and this is followed by a downsampling path of 5 DenseBlocks with skip connections. Then we get a bottleneck,  followed by an upsampling path with 4 non-skipped DenseBlocks and one final DenseBlock which does include a skip connection. Before the final output, the network has one last convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from building_blocks import *\n",
    "\n",
    "def net(input, PARAMS, is_training=True):\n",
    "    net = slim.conv2d(input, PARAMS['input_num_features'],\n",
    "                      3, weights_initializer=weight_initializer,\n",
    "                      weights_regularizer=l2_reg,\n",
    "                      scope='inputConv', activation_fn=None)\n",
    "    dense_down = list()\n",
    "    for i in range(1, 6):\n",
    "        dense_k = skipped_dense_block(net, PARAMS['dense_{}'.format(i)]['num_layers'],\n",
    "            PARAMS['num_features'], 'dense{}'.format(i), is_training=is_training)\n",
    "        net = transition_down(dense_k, 'td{}'.format(i), is_training=is_training)\n",
    "        dense_down.append(dense_k)\n",
    "\n",
    "    net = nonskipped_dense_block(\n",
    "        net, PARAMS['dense_bottleneck']['num_layers'], PARAMS['num_features'], 'denseBottleneck', is_training=is_training)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        net = transition_up(net, 'tu{}'.format(i))\n",
    "        net = tf.concat(\n",
    "            axis=-1, values=[dense_down[-i], net], name=('skip{}_up'.format(i)))\n",
    "        if (i < 5):\n",
    "            net = nonskipped_dense_block(net, PARAMS['dense_{}_up'.format(i)]['num_layers'],\n",
    "                PARAMS['num_features'], 'dense{}up'.format(i), is_training=is_training)\n",
    "        else:\n",
    "            # Last upsampling dense block has a skip connection\n",
    "            net = skipped_dense_block(net, PARAMS['dense_{}_up'.format(i)]['num_layers'],\n",
    "                PARAMS['num_features'], 'dense{}up'.format(i), is_training=is_training)\n",
    "\n",
    "    return slim.conv2d(net, PARAMS['output_classes'], 1,\n",
    "            weights_initializer=weight_initializer, weights_regularizer=l2_reg,\n",
    "            scope='outputConv', activation_fn=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation loop\n",
    "Below, the weights of the trained network are reloaded and the dataset is passed through the network. The results will be visible in TensorBoard from the `test` folder. In there the image tab will show the original photo in the upper left corner, the ground truth in the lower left corner, the difference in the upper right corner (white depicts a wrong prediction), and the predicted segmentation in the lower right corner of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "# Evaluation loop\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    log_dir = 'test'\n",
    "    # We load a batch and reshape to tensor\n",
    "    xbatch, ybatch = batch(\n",
    "        tfsdataset, batch_size=batch_size, height=height, width=width, resized=final_resized)\n",
    "    input_batch = tf.reshape(xbatch, shape=(batch_size, final_resized, final_resized, 3))\n",
    "    ground_truth_batch = tf.reshape(ybatch, shape=(batch_size, final_resized, final_resized, 1))\n",
    "\n",
    "    # Obtain the prediction\n",
    "    predictions = net(input_batch, params_dict, is_training=False)\n",
    "    predim = tf.nn.softmax(predictions)\n",
    "    predimmax = tf.expand_dims(\n",
    "        tf.cast(tf.argmax(predim, axis=3), tf.float32), -1)\n",
    "\n",
    "    yb = tf.cast(tf.divide(ground_truth_batch, 11), tf.float32)\n",
    "    predimmaxdiv = tf.divide(tf.cast(predimmax, tf.float32), 11)\n",
    "\n",
    "    # We calculate the loss\n",
    "    one_hot_labels = slim.one_hot_encoding(\n",
    "        tf.squeeze(ground_truth_batch),\n",
    "        params_dict['output_classes'])\n",
    "\n",
    "    masked_weights = 1 - tf.unstack(one_hot_labels, axis=-1)[-1]\n",
    "\n",
    "    # Concatenate all four images into one grid\n",
    "    ediff = tf.minimum(tf.abs(tf.subtract(yb, predimmaxdiv)), tf.expand_dims(masked_weights, axis=-1))\n",
    "    norm_ediff = tf.ceil(ediff)\n",
    "    annots=tf.concat([colorize(ground_truth_batch),colorize(predimmax)],2)\n",
    "    img_and_err=tf.multiply(tf.concat([input_batch,tf.image.grayscale_to_rgb(norm_ediff)],2),255) # Multiply by 255, because it actually outputs 0.0 to 1.0\n",
    "    aio=tf.concat([img_and_err,annots],1)\n",
    "    tf.summary.image(\"All_in_one\", aio, max_outputs=n_images)\n",
    "    tf.summary.image(\"Legend\", legend, max_outputs=1)\n",
    "\n",
    "    slim.losses.softmax_cross_entropy(\n",
    "        predictions,\n",
    "        one_hot_labels,\n",
    "        weights=masked_weights)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.summary.scalar('loss', total_loss)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(norm_ediff, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    iou_array, mean_iou = intersection_over_union(ground_truth_batch, predimmax, params_dict['output_classes'], masked_weights)\n",
    "    tf.summary.scalar('mean_IoU', mean_iou)\n",
    "    class_labels = tf.convert_to_tensor(np.array(list(_mask_labels.values())), tf.string)\n",
    "    iou_per_class = tf.stack([class_labels, tf.as_string(iou_array, precision=2)], axis=1)\n",
    "    tf.summary.text('IoU per class', iou_per_class)\n",
    "\n",
    "    slim.evaluation.evaluate_once(\n",
    "        '',\n",
    "        'train_aws_nbs/model.ckpt-10963', # Model weights\n",
    "        'test'                            # Save directory for the logs\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
