{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADE Dataset (humans)\n",
    "\n",
    "> Michal Gallus (s172679) Julien Hoareau (s161088) Wazir Sahebali (s172062)\n",
    "\n",
    "In this notebook the main evaluation of the network will be demonstrated on the [ADE dataset](http://sceneparsing.csail.mit.edu/) with the purpose of segmenting humans in images. A large part of this network is the same as the [network we applied on the CamVid dataset](./Main%20Notebook.ipynb). The output will be shown in TensorBoard from the `val_ade` folder. In there the image tab will show the photo in the upper left corner, the ground truth in the lower left corner, the difference in the upper right corner (white depicts a wrong prediction), and the predicted segmentation in the lower right corner of each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can specify the amount of test images you want to go through the model. The maximum amount is 532, as there are only that many images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 300 # total 532"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "We import the main libraries along with the 56 version of the network and its parameters. In this application the network only has two classes, i.e. human and non-human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from network import net\n",
    "import json\n",
    "from data_utils_ADE import *\n",
    "slim = tf.contrib.slim\n",
    "from tensorflow.python.ops import math_ops, array_ops\n",
    "\n",
    "classes=2\n",
    "batch_size, height, width, nchannels = n_images, -1, -1, 3\n",
    "final_resized = 224\n",
    "model_version = 56\n",
    "_mask_labels_ADE = {0: 'nonhuman', 1: 'human'}\n",
    "\n",
    "with open('model_parameters.json') as params:\n",
    "    params_dict = json.load(params)[repr(model_version)]\n",
    "params_dict['input_num_features'] = 48\n",
    "params_dict['output_classes'] = classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the IoU calculation is defined as we did in the CamVid model, but without any masked weights and for 2 classes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(ground_truths, predictions, num_classes):\n",
    "    with tf.variable_scope('iou'):\n",
    "        batch_size = ground_truths.shape[0]\n",
    "        iou_array = list()\n",
    "        for i in range(batch_size):\n",
    "            gt = tf.reshape(ground_truths[i], [-1])\n",
    "            pr = tf.reshape(predictions[i], [-1])\n",
    "            confusion_matrix = tf.confusion_matrix(gt, pr, num_classes, name='cm/' + str(i))\n",
    "            sum_over_row = math_ops.to_float(math_ops.reduce_sum(confusion_matrix, 0))\n",
    "            sum_over_col = math_ops.to_float(math_ops.reduce_sum(confusion_matrix, 1))\n",
    "            cm_diag = math_ops.to_float(array_ops.diag_part(confusion_matrix))\n",
    "            denominator = sum_over_row + sum_over_col - cm_diag\n",
    "            # The mean is only computed over classes that appear in the\n",
    "            # label or prediction tensor. If the denominator is 0, we need to\n",
    "            # ignore the class.\n",
    "            num_valid_entries = math_ops.reduce_sum(math_ops.cast(\n",
    "              math_ops.not_equal(denominator, 0), dtype=tf.float32))\n",
    "            # If the value of the denominator is 0, set it to 1 to avoid\n",
    "            # zero division.\n",
    "            denominator = array_ops.where(\n",
    "              math_ops.greater(denominator, 0),\n",
    "              denominator,\n",
    "              array_ops.ones_like(denominator))\n",
    "            iou = math_ops.div(cm_diag, denominator)\n",
    "            iou_array.append(iou)\n",
    "\n",
    "        iou_array = tf.stack(iou_array)\n",
    "        return tf.reduce_mean(iou_array, axis=0), tf.reduce_mean(iou_array) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "The Tensorflow record file (a container containing the dataset), is actually already in the GitHub folder, so there is no need to recreate it. The code for recreating it is similar to the code in the [main notebook](./Main%20Notebook.ipynb#Dataloading). \n",
    "If the validation set is not yet present as a TensorFlow Record it will be created here and loaded. Note that if you want to recreate the TensorFlow Record file, you have to change the variable `ADEDIR` to the location where the ADE dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetfilename=\"validationset_ADE.tfrec\"\n",
    "if not os.path.isfile(datasetfilename):\n",
    "    from convertimages import *\n",
    "    ADEDIR=\"../ADEChallengeData2016/\" # Change this to where you stored the ADE dataset\n",
    "    allconvert(ADEDIR) # Sets all non-human classes to 0 and the human class to 1 in all images, all images with humans in it are appended to a list\n",
    "    tfrec_dump(\"validation\", datasetfilename, ADEDIR)\n",
    "tfsdataset = slim_dataset(datasetfilename, n_images) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of data transformations applied to the ADE dataset are the same as the ones we applied for the CamVid dataset, i.e. random flipping and random cropping to a resolution of 224 by 224. The one difference is that the ADE dataset contains images of different resolutions, which is why we first resize the smallest side of the image to 224 (maintaining the aspect ratio) and then crop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizecrop_image_and_labels(image, labels, feature_maps_image, feature_maps_annot, height, width):\n",
    "    \"\"\"Resizes and randomly crops `image` together with `labels`.\n",
    "    Based on <https://stackoverflow.com/questions/42147427/tensorflow-how-to-randomly-crop-input-images-and-labels-in-the-same-way>\n",
    "    \"\"\"\n",
    "    seed = random.randint(0, 1e10)\n",
    "    # We concatenate the image with its annotations as an additional feature map\n",
    "    combined = tf.concat([image, labels], axis=-1)\n",
    "    image_shape = tf.shape(image)\n",
    "    \n",
    "    # We have to find out which side is smaller\n",
    "    condition=tf.less(image_shape[0], image_shape[1]) \n",
    "    # About the casting: The multiplication has to happen with floats and the eventual resolution has to be in integers\n",
    "    res_h=tf.cond(condition, lambda: height, lambda: tf.cast(tf.truediv(width,image_shape[1])*tf.cast(image_shape[0], tf.float64), tf.int32))\n",
    "    res_w=tf.cond(condition, lambda: tf.cast(tf.truediv(height,image_shape[0])*tf.cast(image_shape[1], tf.float64), tf.int32), lambda: width)\n",
    "    \n",
    "    # We resize the image\n",
    "    combined_resize = tf.image.resize_images(\n",
    "        combined,\n",
    "        size=[res_h, res_w])\n",
    "        #Default ResizeMethod.BILINEAR\n",
    "    last_label_dim = tf.shape(labels)[-1]\n",
    "    last_image_dim = tf.shape(image)[-1]\n",
    "    # And then we crop the image\n",
    "    combined_crop = tf.random_crop(\n",
    "        combined_resize,\n",
    "        size=[height, width, feature_maps_image + feature_maps_annot],\n",
    "        seed=seed)\n",
    "    combined_crop = tf.reshape(combined_crop, shape=(height, width, feature_maps_image + feature_maps_annot))\n",
    "    maybe_flipped_images = tf.image.random_flip_left_right(combined_crop)\n",
    "    crop_feature_maps = tf.unstack(maybe_flipped_images, axis=-1)\n",
    "    return tf.stack(crop_feature_maps[:feature_maps_image], axis=-1), tf.stack(crop_feature_maps[feature_maps_image:], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Network\n",
    "The network model applied here is exactly the same as in the main notebook. Further explanations of this can thus be found in the [other notebook](./Main%20Notebook.ipynb#The-Network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data loader is set up, the data can be passed through the network. Below, the weights of the trained network are reloaded and the dataset is passed through the network. The results will be visible in TensorBoard from the `val_ade` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "# Loop\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    log_dir = 'val_ade'\n",
    "    # We load a batch and reshape to tensor\n",
    "    xbatch, ybatch = batch(\n",
    "        tfsdataset, batch_size=batch_size, height=height, width=width, resized=final_resized)\n",
    "    input_batch = tf.reshape(xbatch, shape=(batch_size, final_resized, final_resized, 3))\n",
    "    ground_truth_batch = tf.reshape(ybatch, shape=(batch_size, final_resized, final_resized, 1))\n",
    "\n",
    "    # Obtain the prediction\n",
    "    predictions = net(input_batch, params_dict, is_training=False)\n",
    "   \n",
    "    # We calculate the loss\n",
    "    one_hot_labels = slim.one_hot_encoding(\n",
    "        tf.squeeze(ground_truth_batch),\n",
    "        params_dict['output_classes'])\n",
    "    slim.losses.softmax_cross_entropy(\n",
    "        predictions,\n",
    "        one_hot_labels)\n",
    "\n",
    "    # The prediction is softmaxed and the class with the highest probability for each pixel is retained\n",
    "    predim = tf.nn.softmax(predictions)\n",
    "    predimmax = tf.expand_dims(tf.cast(tf.argmax(predim, axis=3), tf.float32), -1)\n",
    "        \n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.summary.scalar('loss', total_loss)\n",
    "    \n",
    "    yb = tf.cast(tf.divide(ground_truth_batch, classes), tf.float32)\n",
    "    \n",
    "    predim = tf.nn.softmax(predictions)\n",
    "    predimmax = tf.expand_dims(\n",
    "        tf.cast(tf.argmax(predim, axis=3), tf.float32), -1)\n",
    "    predimmaxdiv = tf.divide(tf.cast(predimmax, tf.float32), classes)\n",
    "    \n",
    "    # A difference picture is created\n",
    "    ediff = tf.abs(tf.subtract(yb, predimmaxdiv))\n",
    "    norm_ediff = tf.ceil(ediff)\n",
    "    accuracy = tf.reduce_mean(tf.cast(norm_ediff, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # All 4 images are concatenated into 1\n",
    "    annots=tf.concat([tf.multiply(input_batch, yb),tf.multiply(input_batch, predimmaxdiv)],2)\n",
    "    img_and_err=tf.concat([input_batch,tf.image.grayscale_to_rgb(norm_ediff)],2)\n",
    "    aio=tf.concat([img_and_err,annots],1)\n",
    "    tf.summary.image(\"All_in_one\", aio, max_outputs=n_images)\n",
    "    \n",
    "    # The mean IoU is calculated\n",
    "    iou_array, mean_iou = intersection_over_union(ground_truth_batch, predimmax, params_dict['output_classes'])\n",
    "    tf.summary.scalar('mean_IoU', mean_iou)\n",
    "    class_labels = tf.convert_to_tensor(np.array(list(_mask_labels_ADE.values())), tf.string)\n",
    "    iou_per_class = tf.stack([class_labels, tf.as_string(iou_array, precision=2)], axis=1)\n",
    "    tf.summary.text('IoU per class', iou_per_class)\n",
    "\n",
    "    slim.evaluation.evaluate_once(\n",
    "        '',\n",
    "        'train_ade/model.ckpt-46176', # Model weights\n",
    "        log_dir                       # Save directory for the logs\n",
    "    ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
