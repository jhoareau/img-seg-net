{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADE Dataset (humans)\n",
    "\n",
    "> Michal Gallus (s172679) Julien Hoareau (s161088) Wazir Sahebali (s172062)\n",
    "\n",
    "In this notebook the main evaluation of the network will be demonstrated on the ADE dataset with the purpose of segmenting humans in images. The output will be shown in TensorBoard from the `val_ade` folder. In there the image tab will show the photo in the upper left corner, the ground truth in the lower left corner, the difference in the upper right corner (white depicts a wrong prediction), and the predicted segmentation in the lower right corner of each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can specify the amount of test images you want to go through the model. The maximum amount is 532, as there are only that many images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 3 # total 532"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the main libraries along with the 56 version of the network and its parameters. In this application the network only has two classes, i.e. human and non-human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from network import net\n",
    "import json\n",
    "from data_utils_ADE import *\n",
    "slim = tf.contrib.slim\n",
    "from tensorflow.python.ops import math_ops, array_ops\n",
    "\n",
    "classes=2\n",
    "batch_size, height, width, nchannels = n_images, -1, -1, 3\n",
    "final_resized = 224\n",
    "model_version = 56\n",
    "_mask_labels_ADE = {0: 'nonhuman', 1: 'human'}\n",
    "\n",
    "with open('model_parameters.json') as params:\n",
    "    params_dict = json.load(params)[repr(model_version)]\n",
    "params_dict['input_num_features'] = 48\n",
    "params_dict['output_classes'] = classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the IoU calculation is defined as we did in the CamVid model, but without any masked weights and for 2 classes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(ground_truths, predictions, num_classes):\n",
    "    with tf.variable_scope('iou'):\n",
    "        batch_size = ground_truths.shape[0]\n",
    "        iou_array = list()\n",
    "        for i in range(batch_size):\n",
    "            gt = tf.reshape(ground_truths[i], [-1])\n",
    "            pr = tf.reshape(predictions[i], [-1])\n",
    "            confusion_matrix = tf.confusion_matrix(gt, pr, num_classes, name='cm/' + str(i))\n",
    "            sum_over_row = math_ops.to_float(math_ops.reduce_sum(confusion_matrix, 0))\n",
    "            sum_over_col = math_ops.to_float(math_ops.reduce_sum(confusion_matrix, 1))\n",
    "            cm_diag = math_ops.to_float(array_ops.diag_part(confusion_matrix))\n",
    "            denominator = sum_over_row + sum_over_col - cm_diag\n",
    "            # The mean is only computed over classes that appear in the\n",
    "            # label or prediction tensor. If the denominator is 0, we need to\n",
    "            # ignore the class.\n",
    "            num_valid_entries = math_ops.reduce_sum(math_ops.cast(\n",
    "              math_ops.not_equal(denominator, 0), dtype=tf.float32))\n",
    "            # If the value of the denominator is 0, set it to 1 to avoid\n",
    "            # zero division.\n",
    "            denominator = array_ops.where(\n",
    "              math_ops.greater(denominator, 0),\n",
    "              denominator,\n",
    "              array_ops.ones_like(denominator))\n",
    "            iou = math_ops.div(cm_diag, denominator)\n",
    "            iou_array.append(iou)\n",
    "\n",
    "        iou_array = tf.stack(iou_array)\n",
    "        return tf.reduce_mean(iou_array, axis=0), tf.reduce_mean(iou_array) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the validation set is not yet present as a TensorFlow Record it will be created here and loaded. Since the TF Record is already available in the GitHub repository, this is unnecessary. Note that if you want to recreate the TensorFlow Record file, you have to change the variable `ADEDIR` to the location where the ADE dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetfilename=\"validationset_ADE.tfrec\"\n",
    "if not os.path.isfile(datasetfilename):\n",
    "    from convertimages import *\n",
    "    ADEDIR=\"../ADEChallengeData2016/\" # Change this to where you stored the ADE dataset\n",
    "    allconvert(ADEDIR)\n",
    "    tfrec_dump(\"validation\", datasetfilename, ADEDIR)\n",
    "tfsdataset = slim_dataset(datasetfilename, n_images) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data loader is set up, the data can be passed through the network. Below, the weights of the trained network are reloaded and the dataset is passed through the network. The results will be visible in TensorBoard from the `val_ade` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "# Loop\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    log_dir = 'val_ade'\n",
    "    # We load a batch and reshape to tensor\n",
    "    xbatch, ybatch = batch(\n",
    "        tfsdataset, batch_size=batch_size, height=height, width=width, resized=final_resized)\n",
    "    input_batch = tf.reshape(xbatch, shape=(batch_size, final_resized, final_resized, 3))\n",
    "    ground_truth_batch = tf.reshape(ybatch, shape=(batch_size, final_resized, final_resized, 1))\n",
    "\n",
    "    # Obtain the prediction\n",
    "    predictions = net(input_batch, params_dict, is_training=False)\n",
    "   \n",
    "    # We calculate the loss\n",
    "    one_hot_labels = slim.one_hot_encoding(\n",
    "        tf.squeeze(ground_truth_batch),\n",
    "        params_dict['output_classes'])\n",
    "    slim.losses.softmax_cross_entropy(\n",
    "        predictions,\n",
    "        one_hot_labels)\n",
    "\n",
    "    # The prediction is softmaxed and the class with the highest probability for each pixel is retained\n",
    "    predim = tf.nn.softmax(predictions)\n",
    "    predimmax = tf.expand_dims(tf.cast(tf.argmax(predim, axis=3), tf.float32), -1)\n",
    "        \n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.summary.scalar('loss', total_loss)\n",
    "    \n",
    "    yb = tf.cast(tf.divide(ground_truth_batch, classes), tf.float32)\n",
    "    \n",
    "    predim = tf.nn.softmax(predictions)\n",
    "    predimmax = tf.expand_dims(\n",
    "        tf.cast(tf.argmax(predim, axis=3), tf.float32), -1)\n",
    "    predimmaxdiv = tf.divide(tf.cast(predimmax, tf.float32), classes)\n",
    "    \n",
    "    # A difference picture is created\n",
    "    ediff = tf.abs(tf.subtract(yb, predimmaxdiv))\n",
    "    norm_ediff = tf.ceil(ediff)\n",
    "    accuracy = tf.reduce_mean(tf.cast(norm_ediff, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # All 4 images are concatenated into 1\n",
    "    annots=tf.concat([tf.multiply(input_batch, yb),tf.multiply(input_batch, predimmaxdiv)],2)\n",
    "    img_and_err=tf.concat([input_batch,tf.image.grayscale_to_rgb(norm_ediff)],2)\n",
    "    aio=tf.concat([img_and_err,annots],1)\n",
    "    tf.summary.image(\"All_in_one\", aio, max_outputs=n_images)\n",
    "    \n",
    "    # The mean IoU is calculated\n",
    "    iou_array, mean_iou = intersection_over_union(ground_truth_batch, predimmax, params_dict['output_classes'])\n",
    "    tf.summary.scalar('mean_IoU', mean_iou)\n",
    "    class_labels = tf.convert_to_tensor(np.array(list(_mask_labels_ADE.values())), tf.string)\n",
    "    iou_per_class = tf.stack([class_labels, tf.as_string(iou_array, precision=2)], axis=1)\n",
    "    tf.summary.text('IoU per class', iou_per_class)\n",
    "\n",
    "    slim.evaluation.evaluate_once(\n",
    "        '',\n",
    "        'train_ade/model.ckpt-46176', # Model weights\n",
    "        'val_ade'                     # Save directory for the logs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
